# ğŸ“° Business Wire News Scraper API

A FastAPI-based service that scrapes press releases from [Business Wire](https://www.businesswire.com), extracts company tickers, publication dates, and article content, and stores them in MongoDB.

You can trigger this scraper by making a POST request with a list of stock ticker symbols â€” the API will fetch only matching news items.

---

## ğŸš€ Features

- Scrapes the Business Wire newsroom index page
- Extracts:
  - Article heading
  - URL
  - Ticker (e.g. `TDCB`)
  - Publication date
  - Main article content
- Filters news by specific tickers passed in the request
- Deduplicates using MongoDB before inserting
- Runs via `uvicorn` and integrates easily with Airflow or cron

---

## ğŸ“¦ Project Structure

\`\`\`
news_scraper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app setup
â”‚   â”œâ”€â”€ config.py            # Environment configs (API keys, Mongo URI)
â”‚   â”œâ”€â”€ models.py            # Request schema
â”‚   â”œâ”€â”€ services/            # Logic for scraping & API calls
â”‚   â”‚   â”œâ”€â”€ firecrawl.py
â”‚   â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”‚   â””â”€â”€ mongo.py
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ scrape.py        # /run-scrape endpoint
â”œâ”€â”€ run.py                   # Uvicorn launcher
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ .gitignore
\`\`\`

---

## ğŸ› ï¸ Setup

### 1. Clone the repo

\`\`\`bash
git clone https://github.com/your-username/business-wire-scraper.git
cd business-wire-scraper
\`\`\`

### 2. Install dependencies

\`\`\`bash
pip install -r requirements.txt
\`\`\`

### 3. Set environment variables

Create a \`.env\` file or set environment variables manually:

\`\`\`bash
export FIRECRAWL_API_KEY=your-firecrawl-key
export OPENAI_API_KEY=your-openai-key
export MONGO_URI=mongodb+srv://<user>:<pass>@cluster.mongodb.net/
\`\`\`

---

## â–¶ï¸ Running the API

Simply run:

\`\`\`bash
python run.py
\`\`\`

This launches the FastAPI server via \`uvicorn\` on \`http://localhost:8000\`.

---

## ğŸ” API Usage

### Endpoint

\`\`\`
POST /run-scrape
\`\`\`

### Request Body

\`\`\`json
{
  "ticker_codes": ["TDCB", "AAPL", "GOOG"]
}
\`\`\`

### Response

\`\`\`json
{
  "message": "Scrape complete",
  "new_documents_count": 2,
  "new_documents": [
    {
      "heading": "TDCB Reports Q2 Earnings",
      "url": "https://www.businesswire.com/news/home/20250717259715/en/",
      "ticker": "TDCB",
      "date": "Jul 15, 2025 5:00 PM Eastern Daylight Time",
      "content": "Full article text here..."
    }
  ]
}
\`\`\`

---

## ğŸ§ª Testing

You can use [Postman](https://www.postman.com/) or \`curl\`:

\`\`\`bash
curl -X POST http://localhost:8000/run-scrape \
  -H "Content-Type: application/json" \
  -d '{"ticker_codes": ["TDCB"]}'
\`\`\`

---

## ğŸ“… Scheduling with Airflow or Cron

This API is compatible with Astro or Apache Airflow. You can set up a DAG or cron job that calls \`POST /run-scrape\` every 10 minutes with your ticker list.

---

## ğŸ§¹ Ignored Files

Your \`.gitignore\` already excludes:

\`\`\`gitignore
__pycache__/
*.py[cod]
.venv/
.env
\`\`\`

---

## ğŸ“ƒ License

MIT License â€“ use freely with attribution. Contributions welcome!

---

## ğŸ‘¨â€ğŸ’» Author

**Sagar Vaiyata**

Connect on [GitHub](https://github.com/sagarvaiyata) or [LinkedIn](https://linkedin.com/in/sagarvaiyata)
